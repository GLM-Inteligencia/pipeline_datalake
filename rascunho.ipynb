{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from src.common.cloud_storage_connector import CloudStorage\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.common.utils import batch_process, log_process, authenticate, fetch_items_from_storage\n",
    "from src.config import settings\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\n",
    "  \"access_token\": \"APP_USR-2951712600123976-110203-ce78f9cdf280fab258ac0894a9286af2-569119547\",\n",
    "  \"client_id\": \"2951712600123976\",\n",
    "  \"client_secret\": \"QprAIl8ydXzcxFVHjnIHT6fUQ8KpzADV\",\n",
    "  \"seller_id\": 569119547,\n",
    "  \"store_name\": \"gw shop\"\n",
    "}\n",
    "\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Function to extract sku\n",
    "def extract_seller_sku(attributes):\n",
    "    for attribute in attributes:\n",
    "        if attribute.get('id') == 'SELLER_SKU':\n",
    "            return attribute.get('value_name')\n",
    "    return None  \n",
    "\n",
    "# Functions to get relations \n",
    "get_item_relations = lambda x: x.get('item_relations', [])[0].get('id') if len(x.get('item_relations', [])) > 0 else None  \n",
    "get_variation_id_relations = lambda x: x.get('item_relations', [])[0].get('variation_id') if len(x.get('item_relations', [])) > 0 else None  \n",
    "get_stock_relations = lambda x: x.get('item_relations', [])[0].get('stock_relation') if len(x.get('item_relations', [])) > 0 else None  \n",
    "\n",
    "def process_details(content_details, content_variations):  \n",
    "\n",
    "    df_product = pd.DataFrame()\n",
    "\n",
    "        # Checking if item has variations\n",
    "    for item in content_details:\n",
    "\n",
    "        if extract_seller_sku(item.get('attributes', [])):\n",
    "            has_variation = False\n",
    "        else:\n",
    "            has_variation = True \n",
    "\n",
    "        # get channels information\n",
    "        channel = item.get('channels')\n",
    "        flag_marketplace = 'marketplace' in item.get('channels',[])\n",
    "        flag_mshops = 'mshops' in item.get('channels',[])  \n",
    "\n",
    "        # get general information\n",
    "        product_details_general = {\n",
    "            'item_id': item.get('id'),\n",
    "            'item_name': item.get('title'),\n",
    "            'seller_id': item.get('seller_id'),\n",
    "            'category_id': item.get('category_id'),\n",
    "            'official_store_id': item.get('official_store_id'),\n",
    "            'price': item.get('price'),\n",
    "            'base_price': item.get('base_price'),\n",
    "            'original_price': item.get('original_price'),\n",
    "            'initial_quantity': item.get('initial_quantity'),\n",
    "            'status': item.get('status'),\n",
    "            'listing_type': item.get('listing_type_id'),\n",
    "            'url': item.get('permalink'),\n",
    "            'free_shipping': item.get('shipping',{}).get('free_shipping'),\n",
    "            'logistic_type': item.get('shipping',{}).get('logistic_type'),\n",
    "            'catalog_id' : item.get('catalog_product_id'),\n",
    "            'picture_url': item.get('pictures', [{}])[0].get('url'),\n",
    "            'catalog_listing': item.get('catalog_listing', ''),\n",
    "            'item_health': item.get('health',''),\n",
    "            'fg_marketplace': flag_marketplace,\n",
    "            'fg_mshops': flag_mshops,\n",
    "        }  \n",
    "\n",
    "        # If product does not have variations\n",
    "        if not has_variation:\n",
    "            product_detail_variation = {\n",
    "                'inventory_id': item.get('inventory_id'),\n",
    "                'currency_id': item.get('currency_id'),\n",
    "                'stock': item.get('available_quantity'),\n",
    "                'sold_quantity': item.get('sold_quantity'),\n",
    "                'seller_sku': extract_seller_sku(item.get('attributes', [])),\n",
    "                'variation_id': np.nan,\n",
    "                'item_relations': get_item_relations(item),\n",
    "                'stock_relations': get_stock_relations(item),\n",
    "                'variation_id_relations':get_variation_id_relations(item)\n",
    "            }\n",
    "\n",
    "            product_details_general.update(product_detail_variation)\n",
    "            df_ = pd.DataFrame([product_details_general])\n",
    "            df_product = pd.concat([df_product, df_], ignore_index=True)\n",
    "\n",
    "        # If product has variations\n",
    "        else:\n",
    "            for var in item.get('variations', []):\n",
    "                variation_id = var['id']\n",
    "                variation = [variation for variation in content_variations if variation['id'] == variation_id][0]\n",
    "                variation_id = var['id']\n",
    "                product_detail_variation = {\n",
    "                    'inventory_id': variation.get('inventory_id'),\n",
    "                    'currency_id': variation.get('currency_id'),\n",
    "                    'stock': variation.get('available_quantity'),\n",
    "                    'sold_quantity': variation.get('sold_quantity'),\n",
    "                    'seller_sku': extract_seller_sku(variation.get('attributes', [])),\n",
    "                    'variation_id': variation_id,\n",
    "                    'item_relations': get_item_relations(item),\n",
    "                    'stock_relations': get_stock_relations(item),\n",
    "                    'variation_id_relations':get_variation_id_relations(item)\n",
    "                }\n",
    "                product_details_general.update(product_detail_variation)\n",
    "                df_ = pd.DataFrame([product_details_general])\n",
    "                df_product = pd.concat([df_product, df_], ignore_index=True)\n",
    "\n",
    "    return df_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = json\n",
    "store_name = data.get('store_name')\n",
    "seller_id = data.get('seller_id')\n",
    "print('** Connecting to storage and BigQuery... **')\n",
    "# Initialize storage and BigQuery\n",
    "storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "# Define paths and table names from the config\n",
    "bucket_name = settings.BUCKET_STORES\n",
    "table_management = settings.TABLE_MANAGEMENT\n",
    "destiny_table = settings.TABLE_DETAILS\n",
    "blob_details = settings.BLOB_ITEMS_DETAILS(store_name)\n",
    "blob_variations = settings.BLOB_VARIATIONS(store_name)\n",
    "# Define today's date\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "# Get dates to treat\n",
    "list_dates_to_process = bigquery.get_list_dates_to_process(seller_id, table_management, destiny_table)\n",
    "print(f'*** Starting to process dates: {len(list_dates_to_process)} dates to process  ***')\n",
    "df_processed_data = pd.DataFrame()\n",
    "for date in list_dates_to_process:\n",
    "    # Transform date to string\n",
    "    date_to_process = date.strftime('%Y-%m-%d')\n",
    "    print(f'Processing date: {date_to_process}')\n",
    "    # Get blob with the date\n",
    "    blob_prefix_details = blob_details + f'date={date_to_process}/'\n",
    "    blob_prefix_variations = blob_variations + f'date={date_to_process}/'\n",
    "    # List all the files\n",
    "    blobs_details = storage.list_blobs(bucket_name, blob_prefix_details)\n",
    "    blobs_variations = storage.list_blobs(bucket_name, blob_prefix_variations)\n",
    "    \n",
    "    # Empty variables\n",
    "    df_processed_data = pd.DataFrame()\n",
    "    content_details=[]\n",
    "    content_variations=[]\n",
    "    # Getting details data\n",
    "    for blob_det in blobs_details:\n",
    "        # Get content information for details and variations\n",
    "        print(f\"Reading file: {blob_det.name}\")\n",
    "        content_details += storage.download_json(bucket_name, blob_det.name)\n",
    "    # Getting variation data\n",
    "    for blob_var in blobs_variations:\n",
    "        print(f\"Reading file: {blob_var.name}\")\n",
    "        content_variations += storage.download_json(bucket_name, blob_var.name)\n",
    "    df_processed_data = process_details(content_details, content_variations)\n",
    "    df_processed_data['correspondent_date'] = pd.to_datetime(date_to_process)\n",
    "    df_processed_data['process_time'] = datetime.now()\n",
    "    df_processed_data['seller_id'] = seller_id\n",
    "    print(f'*** Finished treating all data. {df_processed_data.shape[0]} products ***')\n",
    "    # print('** Deleting existing data **')\n",
    "    # bigquery.delete_existing_data(destiny_table, seller_id, date_to_process)\n",
    "    \n",
    "    # print('** Correct dataframe schema **')\n",
    "    # bigquery.match_dataframe_schema(df_processed_data, destiny_table)\n",
    "    # print('** Inserting data into BQ**')\n",
    "    # bigquery.insert_dataframe(df_processed_data, destiny_table)\n",
    "    # print('** Updating log table **')\n",
    "    # bigquery.update_logs_table(seller_id, date_to_process, destiny_table, table_management)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = authenticate(json['client_id'], json['client_secret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery.run_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "seller_id = '189643563'\n",
    "url = f\"https://api.mercadolibre.com/users/{seller_id}\"\n",
    "response = requests.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_id = '2000009607285924'\n",
    "\n",
    "order_url = f'https://api.mercadolibre.com/orders/{order_id}'\n",
    "costs_url = f'https://api.mercadolibre.com/orders/{order_id}/costs'\n",
    "\n",
    "headers = a{\n",
    "    'Authorization': f'Bearer {access_token}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.get(order_url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_item_id = 'MLB28017126'\n",
    "\n",
    "url = f\"https://api.mercadolibre.com/products/{catalog_item_id}/items\"\n",
    "\n",
    "# Cabeçalhos de autorização\n",
    "headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "# Fazendo a requisição GET\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 'MLB4966133390'\n",
    "\n",
    "url = f\"https://api.mercadolibre.com/items/{item_id}/shipping\"\n",
    "\n",
    "# Cabeçalhos de autorização\n",
    "headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "# Fazendo a requisição GET\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 'MLB4978023790'\n",
    "\n",
    "url = f\"https://api.mercadolibre.com/items/{item_id}/shipping\"\n",
    "\n",
    "# Cabeçalhos de autorização\n",
    "headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "# Fazendo a requisição GET\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shipping(json_data):\n",
    "    try:\n",
    "        default_value = json_data.get('default')\n",
    "        channels = json_data.get('channels', [])\n",
    "        item_id = json_data.get('item_id')\n",
    "        dict_list = []\n",
    "        for channel in channels:\n",
    "            dict_content = {\n",
    "                'item_id': item_id,\n",
    "                'channel_id': channel.get('id'),\n",
    "                'mode': channel.get('mode'),\n",
    "                'logistic_type': channel.get('logistic_type'),\n",
    "                'local_pick_up': channel.get('local_pick_up'),\n",
    "                'free_shipping': channel.get('free_shipping'),\n",
    "                'store_pick_up': channel.get('store_pick_up'),\n",
    "                'default': default_value\n",
    "            }\n",
    "            dict_list.append(dict_content)\n",
    "        return dict_list\n",
    "    except Exception as e:\n",
    "        print(f'Error processing json: {json_data}, error: {e}')\n",
    "        return []\n",
    "import pandas as pd\n",
    "pd.DataFrame(process_shipping(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json\n",
    "client_id = data.get('client_id')\n",
    "client_secret = data.get('client_secret')\n",
    "store_name = data.get('store_name')\n",
    "seller_id = data.get('seller_id')\n",
    "access_token = data.get('access_token')\n",
    "print('** Defining authentication... **')\n",
    "# Authenticate (assuming this is now centralized in utils.py or a similar file)\n",
    "if not access_token:\n",
    "    access_token = authenticate(client_id, client_secret)  # You can add this to a common module\n",
    "print('** Connecting to storage and BigQuery... **')\n",
    "# Initialize storage and BigQuery\n",
    "storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "# Define paths and table names from the config\n",
    "bucket_name = settings.BUCKET_STORES\n",
    "table_management = settings.TABLE_MANAGEMENT\n",
    "destiny_table = settings.TABLE_FULLFILMENT\n",
    "# Define today's date\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Fetch item IDs from the storage bucket\n",
    "blob_items_prefix = f'{store_name}/meli/api_response/catelog_details/date={today_str}/'\n",
    "items_id = fetch_items_from_storage(\n",
    "storage, \n",
    "bucket_name, \n",
    "blob_items_prefix, \n",
    "key_names='inventory_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from src.common.cloud_storage_connector import CloudStorage\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.config import settings\n",
    "import json\n",
    "\n",
    "\n",
    "def insert_bq_competitors_prices(request):\n",
    "\n",
    "    data = request.get_json()\n",
    "    store_name = data.get('store_name')\n",
    "    seller_id = data.get('seller_id')\n",
    "\n",
    "    print('** Connecting to storage and BigQuery... **')\n",
    "    # Initialize storage and BigQuery\n",
    "    storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "    bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "\n",
    "    # Define paths and table names from the config\n",
    "    bucket_name = settings.BUCKET_STORES\n",
    "    table_management = settings.TABLE_MANAGEMENT\n",
    "    destiny_table = settings.TABLE_CATALOG_COMPETITORS\n",
    "    blob_shipping_cost = settings.BLOB_COMPETITORS_CATALOG(store_name)\n",
    "\n",
    "    # Define today's date\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Get dates to treat\n",
    "    list_dates_to_process = bigquery.get_list_dates_to_process(seller_id, table_management, destiny_table)\n",
    "\n",
    "    print(f'*** Starting to process dates: {len(list_dates_to_process)} dates to process  ***')\n",
    "\n",
    "    df_processed_data = pd.DataFrame()\n",
    "\n",
    "    for date in list_dates_to_process:\n",
    "\n",
    "        # Transform date to string\n",
    "        date_to_process = date.strftime('%Y-%m-%d')\n",
    "        print(f'Processing date: {date_to_process}')\n",
    "        # Get blob with the date\n",
    "        blob_prefix = blob_shipping_cost + f'date={date_to_process}/'\n",
    "        # List all the files\n",
    "        blobs = storage.list_blobs(bucket_name, blob_prefix)\n",
    "\n",
    "        # Processing each blob\n",
    "        for blob in blobs:\n",
    "            print(f\"Reading file: {blob.name}\")\n",
    "            content = storage.download_json(bucket_name, blob.name)\n",
    "\n",
    "            for json in content:\n",
    "                processed_dict = process_prices(json)\n",
    "\n",
    "                if isinstance(processed_dict, list):\n",
    "                    df_processed_data = pd.concat([df_processed_data, pd.DataFrame(processed_dict)], ignore_index = True)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        df_processed_data['correspondent_date'] = pd.to_datetime(date_to_process)\n",
    "        df_processed_data['process_time'] = datetime.now()\n",
    "        df_processed_data['seller_id'] = seller_id\n",
    "\n",
    "        print(f'*** Finished treating all data. {df_processed_data.shape[0]} products ***')\n",
    "\n",
    "        print('** Deleting existing data **')\n",
    "        bigquery.delete_existing_data(destiny_table, seller_id, date_to_process)\n",
    "        \n",
    "        print('** Correct dataframe schema **')\n",
    "        bigquery.match_dataframe_schema(df_processed_data, destiny_table)\n",
    "\n",
    "        print('** Inserting data into BQ**')\n",
    "        bigquery.insert_dataframe(df_processed_data, destiny_table)\n",
    "\n",
    "        print('** Updating log table **')\n",
    "        bigquery.update_logs_table(seller_id, date_to_process, destiny_table, table_management)\n",
    "\n",
    "    return ('Success', 200)\n",
    "\n",
    "\n",
    "def process_prices(json):\n",
    "\n",
    "    try:\n",
    "        extracted_data = []\n",
    "        # Dicionário temporário para priorizar os preços por canal\n",
    "        price_by_channel = {}\n",
    "        for price in json['prices']:\n",
    "            channel = price['conditions']['context_restrictions']\n",
    "            if len(channel) == 1:\n",
    "                channel = channel[0]\n",
    "\n",
    "                # Se ainda não há preço para o canal ou se o preço atual é promoção, atualiza\n",
    "                if channel not in price_by_channel or price['type'] == 'promotion':\n",
    "                    price_by_channel[channel] = {\n",
    "                        'item_id': json.get('id'),\n",
    "                        'price_id': price.get('id'),\n",
    "                        'regular_amount': price.get('regular_amount'),\n",
    "                        'price': price.get('amount'),\n",
    "                        'channel': channel,\n",
    "                        'last_updated': price.get('last_updated')\n",
    "                    }\n",
    "        # Converte os valores armazenados para uma lista\n",
    "        extracted_data.extend(price_by_channel.values())\n",
    "\n",
    "        return extracted_data\n",
    "    \n",
    "    except:\n",
    "        print(f'Error processing json: {json}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = {\n",
    "  \"access_token\": None,\n",
    "  \"client_id\": \"4959083987776428\",\n",
    "  \"client_secret\": \"Hw9wWSydd8PMvMEJewWoMvKGYMAWyKEw\",\n",
    "  \"seller_id\": 189643563,\n",
    "  \"store_name\": \"hubsmarthome\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json\n",
    "store_name = data.get('store_name')\n",
    "seller_id = data.get('seller_id')\n",
    "print('** Connecting to storage and BigQuery... **')\n",
    "# Initialize storage and BigQuery\n",
    "storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "# Define paths and table names from the config\n",
    "bucket_name = settings.BUCKET_STORES\n",
    "table_management = settings.TABLE_MANAGEMENT\n",
    "destiny_table = settings.TABLE_CATALOG_COMPETITORS\n",
    "blob_shipping_cost = settings.BLOB_COMPETITORS_CATALOG(store_name)\n",
    "# Define today's date\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "# Get dates to treat\n",
    "list_dates_to_process = bigquery.get_list_dates_to_process(seller_id, table_management, destiny_table)\n",
    "print(f'*** Starting to process dates: {len(list_dates_to_process)} dates to process  ***')\n",
    "df_processed_data = pd.DataFrame()\n",
    "for date in list_dates_to_process:\n",
    "    # Transform date to string\n",
    "    date_to_process = date.strftime('%Y-%m-%d')\n",
    "    print(f'Processing date: {date_to_process}')\n",
    "    # Get blob with the date\n",
    "    blob_prefix = blob_shipping_cost + f'date={date_to_process}/'\n",
    "    # List all the files\n",
    "    blobs = storage.list_blobs(bucket_name, blob_prefix)\n",
    "    # Processing each blob\n",
    "    for blob in blobs:\n",
    "        print(f\"Reading file: {blob.name}\")\n",
    "        content = storage.download_json(bucket_name, blob.name)\n",
    "        for json in content:\n",
    "            processed_dict = process_competitors_catalog(json)\n",
    "            if isinstance(processed_dict, list):\n",
    "                df_processed_data = pd.concat([df_processed_data, pd.DataFrame(processed_dict)], ignore_index = True)\n",
    "            else:\n",
    "                continue\n",
    "    df_processed_data['correspondent_date'] = pd.to_datetime(date_to_process)\n",
    "    df_processed_data['process_time'] = datetime.now()\n",
    "    df_processed_data['seller_id'] = seller_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0]['item_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = content[0]['results']\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content[0]['results'][0].get('category_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_proc = process_competitors_catalog(content[0])\n",
    "pd.DataFrame(list_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_competitors_catalog(json):\n",
    "\n",
    "    catalog_id = json['item_id']\n",
    "    results_list = []  # Create an empty list to store the dictionaries\n",
    "\n",
    "    try:\n",
    "        for item in json['results']:\n",
    "            dict_content = {\n",
    "                'catalog_product_id': catalog_id, \n",
    "                'item_id' : item.get('item_id'),\n",
    "                'competitors_type': 'catalog',\n",
    "                'category_id': item.get('category_id'),\n",
    "                'official_store_id': item.get('official_store_id'),\n",
    "                'competitor_seller_id': item.get('seller_id'),\n",
    "                'listing_type_id': item.get('listing_type_id'),\n",
    "                'condition': item.get('condition'),\n",
    "            }\n",
    "            \n",
    "            results_list.append(dict_content)  # Append each dictionary to the list\n",
    "        \n",
    "        return results_list  # Return the full list after iterating through all items\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error processing json: {json}. Error: {str(e)}')\n",
    "        return None  # Optionally return None if there's an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve costs\n",
    "data = json = {\n",
    "  \"access_token\": None,\n",
    "  \"client_id\": \"4959083987776428\",\n",
    "  \"client_secret\": \"Hw9wWSydd8PMvMEJewWoMvKGYMAWyKEw\",\n",
    "  \"seller_id\": 189643563,\n",
    "  \"store_name\": \"hubsmarthome\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.cloud_storage_connector import CloudStorage\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.common.utils import batch_process, log_process, authenticate, fetch_items_from_storage\n",
    "from src.config import settings\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "semaphore = asyncio.Semaphore(100)  # Control the number of simultaneous requests\n",
    "\n",
    "\n",
    "# Parsing request data\n",
    "# data = request.get_json()\n",
    "client_id = data.get('client_id')\n",
    "client_secret = data.get('client_secret')\n",
    "store_name = data.get('store_name')\n",
    "seller_id = data.get('seller_id')\n",
    "access_token = data.get('access_token')\n",
    "print('** Defining authentication... **')\n",
    "# Authenticate (assuming this is now centralized in utils.py or a similar file)\n",
    "if not access_token:\n",
    "    access_token = authenticate(client_id, client_secret)  # You can add this to a common module\n",
    "print('** Connecting to storage and BigQuery... **')\n",
    "# Initialize storage and BigQuery\n",
    "storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "# Define paths and table names from the config\n",
    "bucket_name = settings.BUCKET_STORES\n",
    "table_management = settings.TABLE_MANAGEMENT\n",
    "destiny_table = settings.TABLE_COSTS\n",
    "# Define today's date\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "# Getting params to see costs\n",
    "query = f'''\n",
    "    with items_details as (\n",
    "    select distinct\n",
    "        item_id,\n",
    "        listing_type,\n",
    "        category_id\n",
    "    from datalake-v2-424516.datalake_v2.items_details\n",
    "    where\n",
    "        1=1\n",
    "        and date(correspondent_date) = current_date()\n",
    "        and seller_id = {seller_id}\n",
    "    )\n",
    "    select \n",
    "    p.item_id as id,\n",
    "    d.listing_type as listing_type_id,\n",
    "    d.category_id,\n",
    "    p.price,\n",
    "    p.channel\n",
    "    from datalake-v2-424516.datalake_v2.items_prices p\n",
    "    inner join items_details d\n",
    "    on p.item_id = d.item_id\n",
    "    where \n",
    "        1=1\n",
    "        and date(p.correspondent_date) = current_date()\n",
    "        and channel is not null\n",
    "'''\n",
    "# blob_items_prefix = f'{store_name}/meli/api_response/item_detail/date={today_str}/'\n",
    "# items_id = fetch_items_from_storage(\n",
    "# storage, \n",
    "# bucket_name, \n",
    "# blob_items_prefix, \n",
    "# key_names=['id','price', 'category_id', 'listing_type_id']\n",
    "# )\n",
    "\n",
    "df_params = bigquery.run_query(query)\n",
    "# items = df_params[['id','channel']].to_dict(orient='records')\n",
    "df_params['channel'] = df_params['channel'].apply(lambda x : x.replace('channel_', '')).drop(columns = 'channel')\n",
    "items_id = df_params.to_dict(orient='records')\n",
    "\n",
    "print(f'** Items found: {len(items_id)}**')\n",
    "print(f'** Cleaning blob **')\n",
    "# Path for saving \n",
    "blob_basic_path = settings.BLOB_COSTS(store_name)\n",
    "date_blob_path = f'{blob_basic_path}date={today_str}/'\n",
    "# Clean existing files in the storage bucket\n",
    "storage.clean_blobs(bucket_name, date_blob_path)\n",
    "print(f'** Starting API requests for {len(items_id)} items**')\n",
    "# URL function for API\n",
    "url = settings.URL_COST\n",
    "headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "# Batch processing the API requests\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    await batch_process(session, items_id, url, headers, \n",
    "                        bucket_name, date_blob_path, storage, \n",
    "                        params = items_id, add_item_id = True)\n",
    "    \n",
    "log_process(seller_id, destiny_table, today_str, table_management, processed_to_bq=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_process(seller_id, destiny_table, today_str, table_management, processed_to_bq=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.cloud_storage_connector import CloudStorage\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.common.utils import batch_process, log_process, authenticate, fetch_items_from_storage\n",
    "from src.config import settings\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from datetime import datetime\n",
    "\n",
    "semaphore = asyncio.Semaphore(100)  # Control the number of simultaneous requests\n",
    "\n",
    "async def main_async(request):\n",
    "    # Parsing request data\n",
    "    data = request.get_json()\n",
    "    client_id = data.get('client_id')\n",
    "    client_secret = data.get('client_secret')\n",
    "    store_name = data.get('store_name')\n",
    "    seller_id = data.get('seller_id')\n",
    "    access_token = data.get('access_token')\n",
    "\n",
    "    print('** Defining authentication... **')\n",
    "    # Authenticate (assuming this is now centralized in utils.py or a similar file)\n",
    "    if not access_token:\n",
    "        access_token = authenticate(client_id, client_secret)  # You can add this to a common module\n",
    "\n",
    "    print('** Connecting to storage and BigQuery... **')\n",
    "    # Initialize storage and BigQuery\n",
    "    storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "    bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "\n",
    "    # Define paths and table names from the config\n",
    "    bucket_name = settings.BUCKET_STORES\n",
    "    table_management = settings.TABLE_MANAGEMENT\n",
    "    destiny_table = settings.TABLE_VISITS\n",
    "\n",
    "    # Define today's date\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Fetch item IDs from the storage bucket\n",
    "    blob_items_prefix = f'{store_name}/meli/api_response/items/date={today_str}/'\n",
    "    items_id = fetch_items_from_storage(\n",
    "    storage, \n",
    "    bucket_name, \n",
    "    blob_items_prefix, \n",
    "    key_names='results'\n",
    "    )\n",
    "\n",
    "    print(f'** Items found: {len(items_id)}**')\n",
    "\n",
    "    print(f'** Cleaning blob **')\n",
    "    # Path for saving \n",
    "    blob_basic_path = settings.BLOB_VISITS(store_name)\n",
    "    date_blob_path = f'{blob_basic_path}date={today_str}/'\n",
    "\n",
    "    # Clean existing files in the storage bucket\n",
    "    storage.clean_blobs(bucket_name, date_blob_path)\n",
    "\n",
    "    print(f'** Starting API requests for {len(items_id)} items**')\n",
    "    # URL function for API\n",
    "    url = settings.URL_ITEM_DETAIL\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "  \"access_token\": None,\n",
    "  \"client_id\": \"4959083987776428\",\n",
    "  \"client_secret\": \"Hw9wWSydd8PMvMEJewWoMvKGYMAWyKEw\",\n",
    "  \"seller_id\": 189643563,\n",
    "  \"store_name\": \"hubsmarthome\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = args\n",
    "client_id = data.get('client_id')\n",
    "client_secret = data.get('client_secret')\n",
    "store_name = data.get('store_name')\n",
    "seller_id = data.get('seller_id')\n",
    "access_token = data.get('access_token')\n",
    "print('** Defining authentication... **')\n",
    "# Authenticate (assuming this is now centralized in utils.py or a similar file)\n",
    "if not access_token:\n",
    "    access_token = authenticate(client_id, client_secret)  # You can add this to a common module\n",
    "print('** Connecting to storage and BigQuery... **')\n",
    "# Initialize storage and BigQuery\n",
    "storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "# Define paths and table names from the config\n",
    "bucket_name = settings.BUCKET_STORES\n",
    "table_management = settings.TABLE_MANAGEMENT\n",
    "destiny_table = settings.TABLE_VISITS\n",
    "# Define today's date\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Fetch item IDs from the storage bucket\n",
    "blob_items_prefix = f'{store_name}/meli/api_response/items/date={today_str}/'\n",
    "items_id = fetch_items_from_storage(\n",
    "storage, \n",
    "bucket_name, \n",
    "blob_items_prefix, \n",
    "key_names='results'\n",
    ")\n",
    "print(f'** Items found: {len(items_id)}**')\n",
    "print(f'** Cleaning blob **')\n",
    "# Path for saving \n",
    "blob_basic_path = settings.BLOB_VISITS(store_name)\n",
    "date_blob_path = f'{blob_basic_path}date={today_str}/'\n",
    "# Clean existing files in the storage bucket\n",
    "storage.clean_blobs(bucket_name, date_blob_path)\n",
    "print(f'** Starting API requests for {len(items_id)} items**')\n",
    "# URL function for API\n",
    "url = settings.URL_ITEM_DETAIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visits_to_dataframe(json_visit):\n",
    "    # Initialize lists to store the extracted data\n",
    "    \n",
    "    item_id = json_visit['item_id']\n",
    "    visits_data = json_visit['results']\n",
    "    dates = []\n",
    "    total_visits = []\n",
    "    companies = []\n",
    "\n",
    "    # Iterate through the data\n",
    "    for visit in visits_data:\n",
    "        dates.append(visit['date'])\n",
    "        total_visits.append(visit['total'])\n",
    "        company_list = [detail['company'] for detail in visit['visits_detail']]\n",
    "        companies.append(\", \".join(company_list))  # Join company names if there are multiple\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'item_id':item_id,\n",
    "        'date': dates,\n",
    "        'total_visits': total_visits,\n",
    "        'companies': companies\n",
    "    })\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = lambda item_id : f'https://api.mercadolibre.com/items/{item_id}/visits/time_window?last=1&unit=day'\n",
    "headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "item = 'MLB3326162963'\n",
    "response = requests.get(url(item), headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_basic_path = settings.BLOB_VISITS(store_name)\n",
    "bool_first_time = storage.blob_exists(bucket_name, blob_basic_path)\n",
    "bool_first_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# access_token = authenticate(client_id, client_secret)\n",
    "\n",
    "url = lambda item_id : f'https://api.mercadolibre.com/items/{item_id}/visits/time_window?last=150&unit=day&ending=2024-09-07'\n",
    "headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "df_visitas = pd.DataFrame()\n",
    "\n",
    "for i, item in tqdm(enumerate(items_id)):\n",
    "    \n",
    "    response = requests.get(url(item), headers=headers)\n",
    "    print(response.status_code)\n",
    "    daily_visits = response.json()\n",
    "    \n",
    "    df_ = visits_to_dataframe(daily_visits)\n",
    "    \n",
    "    df_visitas = pd.concat([df_visitas, df_], ignore_index=True)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print('Pause')\n",
    "        time.sleep(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from src.common.cloud_storage_connector import CloudStorage\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.config import settings\n",
    "import json\n",
    "\n",
    "\n",
    "def insert_bq_visits(request):\n",
    "\n",
    "    data = request.get_json()\n",
    "    store_name = data.get('store_name')\n",
    "    seller_id = data.get('seller_id')\n",
    "\n",
    "    print('** Connecting to storage and BigQuery... **')\n",
    "    # Initialize storage and BigQuery\n",
    "    storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "    bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "\n",
    "    # Define paths and table names from the config\n",
    "    bucket_name = settings.BUCKET_STORES\n",
    "    table_management = settings.TABLE_MANAGEMENT\n",
    "    destiny_table = settings.TABLE_VISITS\n",
    "    blob_shipping_cost = settings.BLOB_VISITS(store_name)\n",
    "\n",
    "    # Define today's date\n",
    "    today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Get dates to treat\n",
    "    list_dates_to_process = bigquery.get_list_dates_to_process(seller_id, table_management, destiny_table)\n",
    "\n",
    "    print(f'*** Starting to process dates: {len(list_dates_to_process)} dates to process  ***')\n",
    "\n",
    "    df_processed_data = pd.DataFrame()\n",
    "\n",
    "    for date in list_dates_to_process:\n",
    "\n",
    "        # Transform date to string\n",
    "        date_to_process = date.strftime('%Y-%m-%d')\n",
    "        print(f'Processing date: {date_to_process}')\n",
    "        # Get blob with the date\n",
    "        blob_prefix = blob_shipping_cost + f'date={date_to_process}/'\n",
    "        # List all the files\n",
    "        blobs = storage.list_blobs(bucket_name, blob_prefix)\n",
    "\n",
    "        # Processing each blob\n",
    "        for blob in blobs:\n",
    "            print(f\"Reading file: {blob.name}\")\n",
    "            content = storage.download_json(bucket_name, blob.name)\n",
    "\n",
    "            for json in content:\n",
    "                processed_dict = process_shipping(json)\n",
    "\n",
    "                if isinstance(processed_dict, list):\n",
    "                    df_processed_data = pd.concat([df_processed_data, pd.DataFrame(processed_dict)], ignore_index = True)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        df_processed_data['correspondent_date'] = pd.to_datetime(date_to_process)\n",
    "        df_processed_data['process_time'] = datetime.now()\n",
    "        df_processed_data['seller_id'] = seller_id\n",
    "\n",
    "        print(f'*** Finished treating all data. {df_processed_data.shape[0]} products ***')\n",
    "\n",
    "        print('** Deleting existing data **')\n",
    "        bigquery.delete_existing_data(destiny_table, seller_id, date_to_process)\n",
    "        \n",
    "        print('** Correct dataframe schema **')\n",
    "        bigquery.match_dataframe_schema(df_processed_data, destiny_table)\n",
    "\n",
    "        print('** Inserting data into BQ**')\n",
    "        bigquery.insert_dataframe(df_processed_data, destiny_table)\n",
    "\n",
    "        print('** Updating log table **')\n",
    "        bigquery.update_logs_table(seller_id, date_to_process, destiny_table, table_management)\n",
    "\n",
    "    return ('Success', 200)\n",
    "\n",
    "def process_shipping(json_data):\n",
    "    try:\n",
    "        default_value = json_data.get('default')\n",
    "        channels = json_data.get('channels', [])\n",
    "        item_id = json_data.get('item_id')\n",
    "        dict_list = []\n",
    "        for channel in channels:\n",
    "            dict_content = {\n",
    "                'item_id': item_id,\n",
    "                'channel_id': channel.get('id'),\n",
    "                'mode': channel.get('mode'),\n",
    "                'logistic_type': channel.get('logistic_type'),\n",
    "                'local_pick_up': channel.get('local_pick_up'),\n",
    "                'free_shipping': channel.get('free_shipping'),\n",
    "                'store_pick_up': channel.get('store_pick_up'),\n",
    "                'default_shipping': default_value\n",
    "            }\n",
    "            dict_list.append(dict_content)\n",
    "        return dict_list\n",
    "    except Exception as e:\n",
    "        print(f'Error processing json: {json_data}, error: {e}')\n",
    "        return []\n",
    "\n",
    "        \n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=args\n",
    "store_name = data.get('store_name')\n",
    "seller_id = data.get('seller_id')\n",
    "print('** Connecting to storage and BigQuery... **')\n",
    "# Initialize storage and BigQuery\n",
    "storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "# Define paths and table names from the config\n",
    "bucket_name = settings.BUCKET_STORES\n",
    "table_management = settings.TABLE_MANAGEMENT\n",
    "destiny_table = settings.TABLE_VISITS\n",
    "blob_shipping_cost = settings.BLOB_VISITS(store_name)\n",
    "# Define today's date\n",
    "today_str = datetime.today().strftime('%Y-%m-%d')\n",
    "# Get dates to treat\n",
    "list_dates_to_process = bigquery.get_list_dates_to_process(seller_id, table_management, destiny_table)\n",
    "print(f'*** Starting to process dates: {len(list_dates_to_process)} dates to process  ***')\n",
    "df_processed_data = pd.DataFrame()\n",
    "for date in list_dates_to_process:\n",
    "    # Transform date to string\n",
    "    date_to_process = date.strftime('%Y-%m-%d')\n",
    "    print(f'Processing date: {date_to_process}')\n",
    "    # Get blob with the date\n",
    "    blob_prefix = blob_shipping_cost + f'date={date_to_process}/'\n",
    "    # List all the files\n",
    "    blobs = storage.list_blobs(bucket_name, blob_prefix)\n",
    "    # Processing each blob\n",
    "    for blob in blobs:\n",
    "        print(f\"Reading file: {blob.name}\")\n",
    "        content = storage.download_json(bucket_name, blob.name)\n",
    "        for json in content:\n",
    "            processed_dict = process_visits(json)\n",
    "            if isinstance(processed_dict, list):\n",
    "                df_processed_data = pd.concat([df_processed_data, pd.DataFrame(processed_dict)], ignore_index = True)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    df_processed_data['correspondent_date'] = pd.to_datetime(date_to_process)\n",
    "    df_processed_data['process_time'] = datetime.now()\n",
    "    df_processed_data['seller_id'] = seller_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = content[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_visits(json_data):\n",
    "\n",
    "    try:\n",
    "        item_id = json_data.get(\"item_id\")\n",
    "        list_visits = []\n",
    "        for visits_per_date in json_data.get('results',[]):\n",
    "\n",
    "            dict_content = {\n",
    "                \"item_id\": item_id,\n",
    "                \"num_visits\": visits_per_date.get('total'),\n",
    "                \"date\": visits_per_date.get('date')\n",
    "            }\n",
    "\n",
    "            list_visits.append(dict_content)\n",
    "\n",
    "        return list_visits\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing json: {json_data}, error: {e}')\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common.firestore_connector import FirestoreManager\n",
    "from src.config import settings\n",
    "firestore = FirestoreManager(credentials_path=settings.PATH_SERVICE_ACCOUNT, project_id='datalake-meli-dev')\n",
    "\n",
    "firestore.clean_cache('query_cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.config import settings\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def main_fetch_sellers_information():\n",
    "\n",
    "    bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "    table_id = settings.TABLE_SELLER_INFORMATION\n",
    "\n",
    "    # Getting list of sellers to update\n",
    "    query = \"\"\"\n",
    "    WITH sellers_ids AS (\n",
    "        SELECT DISTINCT competitor_seller_id\n",
    "        FROM `datalake-v2-424516.datalake_v2.items_competitors_catalog`\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT DISTINCT competitor_seller_id\n",
    "        FROM `datalake-v2-424516.datalake_v2.items_competitors_details`\n",
    "    )\n",
    "\n",
    "    SELECT DISTINCT si.competitor_seller_id\n",
    "    FROM sellers_ids si \n",
    "    LEFT JOIN `datalake-v2-424516.datalake_v2.sellers_competitors_details` sc\n",
    "    ON CAST(sc.competitor_seller_id AS INT64) = si.competitor_seller_id\n",
    "    WHERE sc.competitor_seller_id IS NULL\n",
    "    \"\"\"\n",
    "\n",
    "    sellers_df = bigquery.run_query(query)\n",
    "    sellers_list = sellers_df['competitor_seller_id'].to_list()\n",
    "\n",
    "    if len(sellers_list) == 0:\n",
    "        print('Zero novos sellers para processar')\n",
    "    \n",
    "    else:\n",
    "        seller_details_list = []\n",
    "        for seller_id in sellers_list:\n",
    "            details = fetch_seller_details(seller_id)\n",
    "            seller_details_list.append(details)\n",
    "\n",
    "        # Creates a dataframe with all the information\n",
    "        print('Creating dataframe')\n",
    "        df_to_save = product_to_save(seller_details_list)\n",
    "\n",
    "        print(f'{df_to_save.shape[0]} sellers encontrados')\n",
    "\n",
    "        # Saving dataframe\n",
    "        print('Match schema dataframe')\n",
    "        df_to_save = bigquery.match_dataframe_schema(df_to_save, table_id)\n",
    "\n",
    "        print('Inserting dataframe')\n",
    "        bigquery.insert_dataframe(df_to_save, table_id)\n",
    "\n",
    "\n",
    "\n",
    "def fetch_seller_details(seller_id):\n",
    "\n",
    "    url = f\"https://api.mercadolibre.com/users/{seller_id}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    seller_data = response.json()\n",
    "\n",
    "    return seller_data\n",
    "    \n",
    "\n",
    "def product_to_save(product_details_list):\n",
    "    competitor_seller_list = []\n",
    "    process_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    for product_data in product_details_list:\n",
    "        if product_data:  \n",
    "\n",
    "            seller_reputation = product_data.get(\"seller_reputation\", {})\n",
    "            transactions = seller_reputation.get(\"transactions\", {})\n",
    "            site_status = product_data.get(\"status\",{})\n",
    "\n",
    "            product_dict = {\n",
    "                'process_time': process_time,\n",
    "                \"competitor_seller_id\": product_data.get(\"id\"),\n",
    "                \"competitor_seller_nickname\": product_data.get(\"nickname\"),\n",
    "                \"competitor_seller_level_id\": seller_reputation.get(\"level_id\", \"\"),\n",
    "                \"competitor_power_seller_status\": seller_reputation.get(\"power_seller_status\", \"\"),\n",
    "                \"competitor_transactions_period\": transactions.get(\"period\", \"\"),\n",
    "                \"competitor_transactions_total\": transactions.get(\"total\", 0),\n",
    "                \"competitor_site_status\": site_status.get(\"site_status\", \"\"), \n",
    "                \"competitor_permalink\": product_data.get(\"permalink\")\n",
    "            }\n",
    "            competitor_seller_list.append(product_dict)\n",
    "\n",
    "    return pd.DataFrame(competitor_seller_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "table_id = settings.TABLE_SELLER_INFORMATION\n",
    "# Getting list of sellers to update\n",
    "query = \"\"\"\n",
    "WITH sellers_ids AS (\n",
    "    SELECT DISTINCT competitor_seller_id\n",
    "    FROM `datalake-v2-424516.datalake_v2.items_competitors_catalog`\n",
    "    UNION ALL\n",
    "    SELECT DISTINCT competitor_seller_id\n",
    "    FROM `datalake-v2-424516.datalake_v2.items_competitors_details`\n",
    ")\n",
    "SELECT DISTINCT si.competitor_seller_id\n",
    "FROM sellers_ids si \n",
    "LEFT JOIN `datalake-v2-424516.datalake_v2.sellers_competitors_details` sc\n",
    "ON CAST(sc.competitor_seller_id AS INT64) = si.competitor_seller_id\n",
    "WHERE sc.competitor_seller_id IS NULL\n",
    "\"\"\"\n",
    "sellers_df = bigquery.run_query(query)\n",
    "sellers_list = sellers_df['competitor_seller_id'].to_list()\n",
    "if len(sellers_list) == 0:\n",
    "    print('Zero novos sellers para processar')\n",
    "\n",
    "else:\n",
    "    seller_details_list = []\n",
    "    for seller_id in sellers_list[:5]:\n",
    "        details = fetch_seller_details(seller_id)\n",
    "        seller_details_list.append(details)\n",
    "    # Creates a dataframe with all the information\n",
    "    print('Creating dataframe')\n",
    "    df_to_save = product_to_save(seller_details_list)\n",
    "    print(f'{df_to_save.shape[0]} sellers encontrados')\n",
    "    # Saving dataframe\n",
    "    print('Match schema dataframe')\n",
    "    df_to_save = bigquery.match_dataframe_schema(df_to_save, table_id)\n",
    "    print('Inserting dataframe')\n",
    "    bigquery.insert_dataframe(df_to_save, table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_seller_id_and_store_name(client_id, client_secret, access_token):\n",
    "    \n",
    "    if not access_token:\n",
    "        print(\"Getting access_token\")\n",
    "        token_url = 'https://api.mercadolibre.com/oauth/token'\n",
    "\n",
    "        token_data = {\n",
    "            'grant_type': 'client_credentials',\n",
    "            'client_id': client_id,\n",
    "            'client_secret': client_secret\n",
    "        }\n",
    "\n",
    "        response = requests.post(token_url, data=token_data)\n",
    "        token_info = response.json()\n",
    "        access_token = token_info['access_token']\n",
    "    \n",
    "    # Step 2: Retrieve User Information\n",
    "    user_info_url = 'https://api.mercadolibre.com/users/me'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}'\n",
    "    }\n",
    "    \n",
    "    user_response = requests.get(user_info_url, headers=headers)\n",
    "    user_info = user_response.json()\n",
    "    \n",
    "    # Extract seller ID and store name\n",
    "    seller_id = user_info['id']\n",
    "    store_name = user_info.get('nickname', 'N/A').split('.')[0]  # Using 'nickname' as store name\n",
    "\n",
    "    return store_name, seller_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access_token = 'TG-673604f2cda3960001605660-1904654004'\n",
    "client_id = '2951712600123976'\n",
    "client_secret = 'QprAIl8ydXzcxFVHjnIHT6fUQ8KpzADV'\n",
    "\n",
    "get_seller_id_and_store_name(client_id, client_secret, access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.mercadolibre.com/oauth/token\"\n",
    "\n",
    "payload = {\n",
    "    \"grant_type\": \"refresh_token\",\n",
    "    \"client_id\": f\"{client_id}\",\n",
    "    \"client_secret\": f\"{client_secret}\",\n",
    "    \"refresh_token\": f\"{access_token}\"\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "response = requests.post(url, data=payload, headers=headers)\n",
    "tokens = response.json()\n",
    "access_token = tokens.get(\"access_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.config import settings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local credentials from: C:/Users/User/Documents/papa preco/service account/service_account_datalakev2.json\n"
     ]
    }
   ],
   "source": [
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import String, Integer, Float, DateTime\n",
    "import numpy as np\n",
    "from urllib.parse import quote_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = quote_plus('Glm@mysql24')  # Your actual password\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(f'mysql+pymysql://geraldo-papa:{password}@34.123.250.92/glm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'item_id' updated successfully!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Database connection\n",
    "password = quote_plus('Glm@mysql24')\n",
    "engine = create_engine(f'mysql+pymysql://geraldo-papa:{password}@34.123.250.92/glm')\n",
    "\n",
    "# Increase VARCHAR size or set it to TEXT\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"ALTER TABLE suggested_items MODIFY COLUMN item_id TEXT;\"))\n",
    "\n",
    "print(\"Column 'item_id' updated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela: competitors / Tamanho em memória: 95.85 MB\n",
      "Tempo decorrido: 119.23 segundos\n",
      "-----------------------------------\n",
      "Tabela: general / Tamanho em memória: 57.25 MB\n",
      "Tempo decorrido: 75.13 segundos\n",
      "-----------------------------------\n",
      "Tabela: performance_table / Tamanho em memória: 51.35 MB\n",
      "Tempo decorrido: 46.08 segundos\n",
      "-----------------------------------\n",
      "Tabela: stock_seller / Tamanho em memória: 19.53 MB\n",
      "Tempo decorrido: 35.31 segundos\n",
      "-----------------------------------\n",
      "Tabela: suggested_items / Tamanho em memória: 0.01 MB\n",
      "Tempo decorrido: 0.62 segundos\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tables_list = ['competitor', 'general', 'performance_table', 'stock_seller', 'suggested_items']\n",
    "\n",
    "for table_name in tables_list:\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(f\"TRUNCATE TABLE {table_name};\"))\n",
    "\n",
    "    if table_name == 'competitor':\n",
    "        table_name = 'competitors'\n",
    "        \n",
    "    df= bigquery.run_query(f'select * from datalake-v2-424516.tables_frontend.{table_name}')\n",
    "    df['created_at'] = datetime.now()\n",
    "    df['updated_at'] = datetime.now()\n",
    "\n",
    "    memory_usage = df.memory_usage(deep=True).sum()/ (1024 ** 2)\n",
    "    print(f\"Tabela: {table_name} / Tamanho em memória: {memory_usage:.2f} MB\" )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    df.to_sql(\n",
    "            name=table_name,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            chunksize=1000,\n",
    "            method='multi',\n",
    "            # dtype=data_types  # Specify data types\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Tempo decorrido: {elapsed_time:.2f} segundos\")\n",
    "    print('-----------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tables_list = ['competitors', 'general', 'performance_table', 'stock_seller', 'suggested_items']\n",
    "tables_list = ['general']\n",
    "index_list = {\n",
    "    'competitors': ['channel', 'glm_id', 'seller_id', 'seller_sku'],\n",
    "    'general': ['glm_id', 'seller_id', 'seller_sku', 'item_id'],\n",
    "    'performance_table': ['channel', 'seller_id', 'item_id'],\n",
    "    'stock_seller': ['glm_id', 'seller_id', 'seller_sku'],\n",
    "    'suggested_items': ['seller_sku'],\n",
    "}\n",
    "\n",
    "for table in tables_list:\n",
    "    \n",
    "    df= bigquery.run_query(f'select * from datalake-v2-424516.tables_frontend.{table}')\n",
    "    df['created_at'] = datetime.now()\n",
    "    df['updated_at'] = datetime.now()\n",
    "    memory_usage = df.memory_usage(deep=True).sum()/ (1024 ** 2)\n",
    "    print(f\"Tabela: {table} / Tamanho em memória: {memory_usage:.2f} MB\" )\n",
    "\n",
    "    start_time = time.time()\n",
    "    upload_data_to_mysql(df, table_name= f'{table}', index_list= index_list[table])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Tempo decorrido: {elapsed_time:.2f} segundos\")\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= bigquery.run_query(f'select * from datalake-v2-424516.tables_frontend.{table_name}')\n",
    "df['created_at'] = datetime.now()\n",
    "df['updated_at'] = datetime.now()\n",
    "\n",
    "df.head(15).to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        if_exists='append',\n",
    "        index=False,\n",
    "        chunksize=1000,\n",
    "        method='multi',\n",
    "        # dtype=data_types  # Specify data types\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import String, Integer, Float, DateTime\n",
    "import numpy as np\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "def upload_data_to_mysql(df, table_name, index_list=None):\n",
    "    # Replace pandas.NA and np.nan with None\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    password = quote_plus('Glm@mysql24')  # Your actual password\n",
    "\n",
    "    # Create the SQLAlchemy engine\n",
    "    engine = create_engine(f'mysql+pymysql://geraldo-papa:{password}@34.123.250.92/glm')\n",
    "\n",
    "    # Define data types for columns\n",
    "    data_types = {}\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            data_types[col] = String(255)  # Set VARCHAR(255) for object columns\n",
    "        elif pd.api.types.is_integer_dtype(df[col].dtype):\n",
    "            data_types[col] = Integer()\n",
    "        elif pd.api.types.is_float_dtype(df[col].dtype):\n",
    "            data_types[col] = Float()\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col].dtype):\n",
    "            data_types[col] = DateTime()\n",
    "\n",
    "    # Upload the data to MySQL with specified data types\n",
    "    df.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine,\n",
    "        if_exists='append',\n",
    "        index=False,\n",
    "        chunksize=1000,\n",
    "        method='multi',\n",
    "        dtype=data_types  # Specify data types\n",
    "    )\n",
    "\n",
    "    # # Add indexes to specified columns\n",
    "    # if index_list:\n",
    "    #     with engine.connect() as conn:\n",
    "    #         for index_column in index_list:\n",
    "    #             if index_column in df.columns and df[index_column].dtype == object:\n",
    "    #                 # Specify a key length for VARCHAR/TEXT columns\n",
    "    #                 conn.execute(\n",
    "    #                     text(f'CREATE INDEX idx_{index_column} ON {table_name} ({index_column}(255));')\n",
    "    #                 )\n",
    "    #             else:\n",
    "    #                 # Create index for other types without key length\n",
    "    #                 conn.execute(\n",
    "    #                     text(f'CREATE INDEX idx_{index_column} ON {table_name} ({index_column});')\n",
    "    #                 )\n",
    "\n",
    "    print(\"Data uploaded and indexes added!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tables_list = ['competitors', 'general', 'performance_table', 'stock_seller', 'suggested_items']\n",
    "tables_list = ['general']\n",
    "index_list = {\n",
    "    'competitors': ['channel', 'glm_id', 'seller_id', 'seller_sku'],\n",
    "    'general': ['glm_id', 'seller_id', 'seller_sku', 'item_id'],\n",
    "    'performance_table': ['channel', 'seller_id', 'item_id'],\n",
    "    'stock_seller': ['glm_id', 'seller_id', 'seller_sku'],\n",
    "    'suggested_items': ['seller_sku'],\n",
    "}\n",
    "\n",
    "for table in tables_list:\n",
    "    \n",
    "    df= bigquery.run_query(f'select * from datalake-v2-424516.tables_frontend.{table}')\n",
    "    df['created_at'] = datetime.now()\n",
    "    df['updated_at'] = datetime.now()\n",
    "    memory_usage = df.memory_usage(deep=True).sum()/ (1024 ** 2)\n",
    "    print(f\"Tabela: {table} / Tamanho em memória: {memory_usage:.2f} MB\" )\n",
    "\n",
    "    start_time = time.time()\n",
    "    upload_data_to_mysql(df, table_name= f'{table}', index_list= index_list[table])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Tempo decorrido: {elapsed_time:.2f} segundos\")\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def recreate_table(df):\n",
    "    # Map pandas dtypes to MySQL data types\n",
    "    dtype_mapping = {\n",
    "        'int64': 'BIGINT',\n",
    "        'float64': 'DOUBLE',\n",
    "        'object': 'TEXT',\n",
    "        'datetime64[ns]': 'DATETIME',\n",
    "        'bool': 'BOOLEAN'\n",
    "    }\n",
    "\n",
    "    # Build the CREATE TABLE statement\n",
    "    columns = df.columns.tolist()\n",
    "    sql_types = []\n",
    "    for col in columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        sql_type = dtype_mapping.get(dtype, 'TEXT')  # Default to TEXT if dtype not found\n",
    "        sql_types.append(f\"`{col}` {sql_type}\")\n",
    "\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS test_general (\n",
    "        {', '.join(sql_types)}\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to MySQL\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"34.123.250.92\",\n",
    "        user=\"geraldo-papa\",\n",
    "        password=\"Glm@mysql24\",\n",
    "        database=\"test_general_table\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # Drop the table if it exists\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS test_general;\")\n",
    "        print(\"Existing table dropped.\")\n",
    "\n",
    "        # Create the new table\n",
    "        cursor.execute(create_table_query)\n",
    "        print(\"New table created with the following schema:\")\n",
    "        print(create_table_query)\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Error: {}\".format(err))\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "def upload_data_to_mysql(df):\n",
    "    # Replace pandas.NA and np.nan with None\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Connect to MySQL with the specified database\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"34.123.250.92\",\n",
    "        user=\"geraldo-papa\",\n",
    "        password=\"Glm@mysql24\",\n",
    "        database=\"test_general_table\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Prepare the INSERT query\n",
    "    insert_query = (\n",
    "        \"INSERT INTO test_general (\" +\n",
    "        \", \".join(f\"`{col}`\" for col in columns) +\n",
    "        \") VALUES (\" +\n",
    "        \", \".join([\"%s\"] * len(columns)) +\n",
    "        \")\"\n",
    "    )\n",
    "\n",
    "    # Convert DataFrame rows to list of tuples\n",
    "    data_to_insert = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_values = []\n",
    "        for col in columns:\n",
    "            value = row[col]\n",
    "            if pd.isna(value):\n",
    "                value = None\n",
    "            row_values.append(value)\n",
    "        data_to_insert.append(tuple(row_values))\n",
    "\n",
    "    try:\n",
    "        # Insert data into MySQL\n",
    "        cursor.executemany(insert_query, data_to_insert)\n",
    "\n",
    "        conn.commit()  # Execute commit after all insertions\n",
    "        print(\"Data uploaded successfully to MySQL.\")\n",
    "    except mysql.connector.Error as err:\n",
    "        print(\"Error: {}\".format(err))\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "# Usage\n",
    "recreate_table(df)\n",
    "upload_data_to_mysql(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data_to_mysql(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
