{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from src.common.cloud_storage_connector import CloudStorage\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.config import settings\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'aiofiles' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01maiofiles\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01maio\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43maio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m \n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'aiofiles' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import aiofiles as aio\n",
    "aio.__version__ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\n",
    "  \"access_token\": None,\n",
    "  \"client_id\": \"4959083987776428\",\n",
    "  \"client_secret\": \"Hw9wWSydd8PMvMEJewWoMvKGYMAWyKEw\",\n",
    "  \"seller_id\": 189643563,\n",
    "  \"store_name\": \"hubsmarthome\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_date(date, storage, bucket_name, blob_shipping_cost):\n",
    "    try:\n",
    "        print(f'Processing date: {date}')\n",
    "        blob_prefix = blob_shipping_cost + f'date={date}/'\n",
    "\n",
    "        # Use run_in_executor to call synchronous methods without blocking\n",
    "        loop = asyncio.get_event_loop()\n",
    "        blobs = await loop.run_in_executor(None, storage.list_blobs, bucket_name, blob_prefix)\n",
    "\n",
    "        df_processed_data = pd.DataFrame()\n",
    "\n",
    "        for blob in blobs:\n",
    "            print(f\"Reading file: {blob.name}\")\n",
    "            content = await loop.run_in_executor(None, storage.download_json, bucket_name, blob.name)\n",
    "\n",
    "            for json_content in content:\n",
    "                df_ = await process_orders(json_content['results'])\n",
    "                df_processed_data = pd.concat([df_processed_data, df_], ignore_index=True)\n",
    "\n",
    "        if not df_processed_data.empty:\n",
    "            df_processed_data['processed_json'] = pd.to_datetime(date)\n",
    "            df_processed_data['process_time'] = datetime.now()\n",
    "\n",
    "        print(f'*** Finished treating all data for date {date}. {df_processed_data.shape[0]} sales ***')\n",
    "\n",
    "        return df_processed_data\n",
    "    except Exception as e:\n",
    "        print(f'Error processing date {date}: {e}')\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_orders(json_data):\n",
    "    try:\n",
    "        df_ = pd.DataFrame()\n",
    "\n",
    "        for sale in json_data:\n",
    "            structured_sale = {\n",
    "                # Your structured_sale dictionary here\n",
    "            }\n",
    "\n",
    "            # Create a new DataFrame for the structured sale\n",
    "            df_sale = pd.DataFrame([structured_sale])\n",
    "\n",
    "            # Concatenate only if df_ is not empty\n",
    "            if df_.empty:\n",
    "                df_ = df_sale  # Assign the first entry directly if df_ is empty\n",
    "            else:\n",
    "                df_ = pd.concat([df_, df_sale], ignore_index=True)\n",
    "\n",
    "        return df_\n",
    "    except Exception as e:\n",
    "        print(f'Error processing json: {e}')\n",
    "        return pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_data\n",
    "store_name = data.get('store_name')\n",
    "seller_id = data.get('seller_id')\n",
    "\n",
    "print('** Connecting to storage and BigQuery... **')\n",
    "# Initialize storage and BigQuery\n",
    "storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "# Define paths and table names from the config\n",
    "bucket_name = settings.BUCKET_STORES\n",
    "table_management = settings.TABLE_MANAGEMENT\n",
    "destiny_table = settings.TABLE_ORDERS\n",
    "blob_shipping_cost = settings.BLOB_ORDERS(store_name)\n",
    "# Get dates to treat\n",
    "list_dates_to_process = bigquery.get_list_dates_to_process(seller_id, table_management, destiny_table)\n",
    "list_dates_to_process = [date.strftime('%Y-%m-%d') for date in list_dates_to_process]\n",
    "print(f'*** Starting to process dates: {len(list_dates_to_process)} dates to process  ***')\n",
    "# Use asyncio.gather to process dates asynchronously\n",
    "tasks = [process_date(date, storage, bucket_name, blob_shipping_cost) for date in list_dates_to_process]\n",
    "results = await asyncio.gather(*tasks)\n",
    "# Combine all DataFrames\n",
    "df_all_processed_data = pd.concat(results, ignore_index=True)\n",
    "print(f'*** Finished processing all dates. Total sales: {df_all_processed_data.shape[0]} ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date: 2024-04-27\n",
      "Reading file: hubsmarthome/meli/api_response/orders/date=2024-04-27/total_sales=384__data=2024-04-27__processing-time=2024-09-17T17:42:42.561756-03:00.json\n",
      "*** Finished treating all data for date 2024-04-27. 8 sales ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = list_dates_to_process[3]\n",
    "print(f'Processing date: {date}')\n",
    "blob_prefix = blob_shipping_cost + f'date={date}/'\n",
    "# Use run_in_executor to call synchronous methods without blocking\n",
    "loop = asyncio.get_event_loop()\n",
    "blobs = await loop.run_in_executor(None, storage.list_blobs, bucket_name, blob_prefix)\n",
    "df_processed_data = pd.DataFrame()\n",
    "for blob in blobs:\n",
    "    print(f\"Reading file: {blob.name}\")\n",
    "    content = await loop.run_in_executor(None, storage.download_json, bucket_name, blob.name)\n",
    "    for json_content in content:\n",
    "        df_ = process_orders(json_content['results'])\n",
    "        df_processed_data = pd.concat([df_processed_data, df_], ignore_index=True)\n",
    "if not df_processed_data.empty:\n",
    "    df_processed_data['processed_json'] = pd.to_datetime(date)\n",
    "    df_processed_data['process_time'] = datetime.now()\n",
    "print(f'*** Finished treating all data for date {date}. {df_processed_data.shape[0]} sales ***')\n",
    "\n",
    "df_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.DataFrame()\n",
    "for sale in json_content:\n",
    "    structured_sale = {\n",
    "        # Your structured_sale dictionary here\n",
    "    }\n",
    "    # Create a new DataFrame for the structured sale\n",
    "    df_sale = pd.DataFrame([structured_sale])\n",
    "    # Concatenate only if df_ is not empty\n",
    "    if df_.empty:\n",
    "        df_ = df_sale  # Assign the first entry directly if df_ is empty\n",
    "    else:\n",
    "        df_ = pd.concat([df_, df_sale], ignore_index=True)\n",
    "\n",
    "structured_sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_orders(json_content['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch historical orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import aiofiles\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from flask import escape\n",
    "import requests\n",
    "import gc  # Garbage collection to free memory\n",
    "from src.common.cloud_storage_connector import CloudStorage\n",
    "from src.common.bigquery_connector import BigQueryManager\n",
    "from src.common.utils import log_process, authenticate\n",
    "from src.config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "timezone_offset = \"-03:00\"\n",
    "limit = 50\n",
    "\n",
    "def fetch_data(date, offset, access_token, seller_id):\n",
    "    formatted_date_start = date.strftime(f\"%Y-%m-%dT00:00:00.000{timezone_offset}\")\n",
    "    formatted_date_end = (date + timedelta(days=1)).strftime(f\"%Y-%m-%dT00:00:00.000{timezone_offset}\")\n",
    "    params = {\n",
    "        'limit': limit,\n",
    "        'offset': offset,\n",
    "        'order.date_created.from': formatted_date_start,\n",
    "        'order.date_created.to': formatted_date_end\n",
    "    }\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}'\n",
    "    }\n",
    "\n",
    "    url = settings.URL_ORDERS(seller_id)\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers) \n",
    "    print(response.json)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = test_data\n",
    "client_id = data.get('client_id')\n",
    "client_secret = data.get('client_secret')\n",
    "store_name = data.get('store_name')\n",
    "seller_id = data.get('seller_id')\n",
    "access_token = data.get('access_token')\n",
    "print('** Defining authentication... **')\n",
    "# Authenticate (assuming this is now centralized in utils.py or a similar file)\n",
    "if not access_token:\n",
    "    access_token = authenticate(client_id, client_secret)  # You can add this to a common module\n",
    "print('** Connecting to storage and BigQuery... **')\n",
    "# Initialize storage and BigQuery\n",
    "storage = CloudStorage(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "bigquery = BigQueryManager(credentials_path=settings.PATH_SERVICE_ACCOUNT)\n",
    "# Define paths and table names from the config\n",
    "bucket_name = settings.BUCKET_STORES\n",
    "table_management = settings.TABLE_MANAGEMENT\n",
    "destiny_table = settings.TABLE_ORDERS\n",
    "start_date = datetime.today() - timedelta(days=365)\n",
    "end_date = datetime.today() - timedelta(days=1)\n",
    "blob_name = settings.BLOB_ORDERS(store_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "responses = []\n",
    "sales = []\n",
    "date = end_date\n",
    "# while True:\n",
    "response = fetch_data(date, offset, access_token, seller_id)\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'process_time', 'processed_json'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Provided dictionary keys from the code\n",
    "code_keys = [\n",
    "    'reason', 'status_code', 'total_paid_amount', 'operation_type', 'transaction_amount', \n",
    "    'transaction_amount_refunded', 'date_approved', 'collector_id', 'coupon_id', 'installments',\n",
    "    'authorization_code', 'taxes_amount', 'payment_id', 'date_last_modified', 'coupon_amount', \n",
    "    'payment_shipping_cost', 'installment_amount', 'activation_uri', 'overpaid_amount', 'card_id', \n",
    "    'payment_status_detail', 'issuer_id', 'payment_method_id', 'payment_type', 'deferred_period', \n",
    "    'atm_transfer_reference_transaction_id', 'atm_transfer_reference_company_id', 'site_id', \n",
    "    'payer_id', 'order_id', 'currency_id', 'payment_status', 'shipping_id', 'fulfilled', 'seller_id', \n",
    "    'buyer_id', 'item_id', 'item_title', 'item_category_id', 'item_variation_id', 'seller_custom_field', \n",
    "    'global_price', 'net_weight', 'warranty', 'condition', 'seller_sku', 'quantity', 'unit_price', \n",
    "    'full_unit_price', 'manufacturing_days', 'requested_quantity_measure', 'requested_quantity_value', \n",
    "    'sale_fee', 'listing_type_id', 'base_exchange_rate', 'base_currency_id', 'bundle', 'element_id', \n",
    "    'date_created', 'date_closed', 'status', 'expiration_date', 'date_last_updated', 'last_updated', \n",
    "    'comment', 'pack_id', 'coupon_amount', 'coupon_id', 'shipping_cost', 'pickup_id', 'status_detail', \n",
    "    'total_amount', 'paid_amount', 'context_application', 'context_product_id', 'context_channel', \n",
    "    'context_site'\n",
    "]\n",
    "\n",
    "# Keys from the schema\n",
    "schema_keys = [\n",
    "    'reason', 'status_code', 'total_paid_amount', 'operation_type', 'transaction_amount',\n",
    "    'transaction_amount_refunded', 'date_approved', 'collector_id', 'coupon_id', 'installments',\n",
    "    'authorization_code', 'taxes_amount', 'payment_id', 'date_last_modified', 'coupon_amount', \n",
    "    'shipping_cost', 'installment_amount', 'activation_uri', 'overpaid_amount', 'card_id', \n",
    "    'status_detail', 'issuer_id', 'payment_method_id', 'payment_type', 'deferred_period', \n",
    "    'atm_transfer_reference_transaction_id', 'atm_transfer_reference_company_id', 'site_id', \n",
    "    'payer_id', 'order_id', 'currency_id', 'payment_status', 'shipping_id', 'fulfilled', 'seller_id', \n",
    "    'buyer_id', 'item_id', 'item_title', 'item_category_id', 'item_variation_id', 'seller_custom_field', \n",
    "    'global_price', 'net_weight', 'warranty', 'condition', 'seller_sku', 'quantity', 'unit_price', \n",
    "    'full_unit_price', 'manufacturing_days', 'requested_quantity_measure', 'requested_quantity_value', \n",
    "    'sale_fee', 'listing_type_id', 'base_exchange_rate', 'base_currency_id', 'bundle', 'element_id', \n",
    "    'date_created', 'date_closed', 'status', 'expiration_date', 'date_last_updated', 'last_updated', \n",
    "    'comment', 'pack_id', 'pickup_id', 'total_amount', 'paid_amount', 'context_application', \n",
    "    'context_product_id', 'context_channel', 'context_site', 'processed_json', 'process_time'\n",
    "]\n",
    "\n",
    "# Find keys that are in the dictionary but not in the schema\n",
    "extra_keys = set(code_keys) - set(schema_keys)\n",
    "extra_keys = set(schema_keys) - set(code_keys)  \n",
    "\n",
    "extra_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'process_time', 'processed_json'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_keys_ = set(code_keys) - set(schema_keys)\n",
    "extra_keys = set(schema_keys) - set(code_keys)  \n",
    "extra_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'payment_shipping_cost', 'payment_status_detail'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_keys_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
